{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RAG Chatbot on Google Colab (Demo Version)\n",
    "\n",
    "# ✅ STEP 1: Install Dependencies\n",
    "!pip install faiss-cpu scikit-learn PyPDF2 langchain transformers streamlit-ngrok -q\n",
    "\n",
    "# ✅ STEP 2: Upload your PDF\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "import PyPDF2\n",
    "pdf_text = \"\"\n",
    "for filename in uploaded:\n",
    "    reader = PyPDF2.PdfReader(open(filename, \"rb\"))\n",
    "    for page in reader.pages:\n",
    "        pdf_text += page.extract_text()\n",
    "\n",
    "# ✅ STEP 3: Chunk the text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_text(pdf_text)\n",
    "\n",
    "# ✅ STEP 4: Embed with TF-IDF + Index with FAISS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(chunks).toarray().astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "index.add(vectors)\n",
    "\n",
    "# ✅ STEP 5: Define query+retrieval+mock LLM response\n",
    "def get_top_k_chunks(query, k=3):\n",
    "    query_vec = vectorizer.transform([query]).toarray().astype(\"float32\")\n",
    "    D, I = index.search(query_vec, k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def mock_generate_response(context_chunks, query):\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    return f\"**Your Query:** {query}\\n\\n**Contextual Answer:**\\n- {context[:300]}...\\n\\n(Structured response simulated here)\"\n",
    "\n",
    "# ✅ STEP 6: Interactive input\n",
    "while True:\n",
    "    query = input(\"Ask a question (or type 'exit'): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    top_chunks = get_top_k_chunks(query)\n",
    "    print(mock_generate_response(top_chunks, query))\n",
    "\n",
    "# ✅ BONUS: Run Streamlit in Colab (Optional)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
