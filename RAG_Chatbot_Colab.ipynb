{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üî¨ Scientific Paper RAG Chatbot - Google Colab Demo\n",
    "# Features: Section-Level Semantic Querying + Rank-Based Re-weighting\n",
    "\n",
    "#  STEP 1: Install Dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install faiss-cpu scikit-learn PyPDF2 langchain sentence-transformers openai -q\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "\n",
    "#  STEP 2: Upload your Scientific Paper PDF\n",
    "print(\"\\nüìÑ Please upload your scientific paper PDF\")\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "#  STEP 3: Extract text and detect sections\n",
    "import PyPDF2\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Section patterns for scientific papers\n",
    "SECTION_PATTERNS = [\n",
    "    r'^\\s*abstract\\s*$', r'^\\s*introduction\\s*$', r'^\\s*background\\s*$',\n",
    "    r'^\\s*related\\s+work\\s*$', r'^\\s*methodology\\s*$', r'^\\s*methods\\s*$',\n",
    "    r'^\\s*results\\s*$', r'^\\s*discussion\\s*$', r'^\\s*conclusion\\s*$',\n",
    "    r'^\\s*references\\s*$', r'^\\s*acknowledgments?\\s*$'\n",
    "]\n",
    "\n",
    "def detect_sections(text):\n",
    "    \"\"\"Detect scientific paper sections\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    sections = []\n",
    "    current_section = {'name': 'Header', 'start_line': 0, 'content': ''}\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_lower = line.lower().strip()\n",
    "        is_section_header = False\n",
    "        \n",
    "        for pattern in SECTION_PATTERNS:\n",
    "            if re.match(pattern, line_lower, re.IGNORECASE):\n",
    "                if current_section['content'].strip():\n",
    "                    sections.append(current_section)\n",
    "                current_section = {'name': line.strip().title(), 'start_line': i, 'content': ''}\n",
    "                is_section_header = True\n",
    "                break\n",
    "        \n",
    "        if not is_section_header:\n",
    "            current_section['content'] += line + '\\n'\n",
    "    \n",
    "    if current_section['content'].strip():\n",
    "        sections.append(current_section)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "print(\"\\nüîç Extracting text and detecting sections...\")\n",
    "pdf_text = \"\"\n",
    "for filename in uploaded:\n",
    "    reader = PyPDF2.PdfReader(open(filename, \"rb\"))\n",
    "    for page_num, page in enumerate(reader.pages):\n",
    "        page_text = page.extract_text()\n",
    "        pdf_text += f\"\\n[PAGE {page_num + 1}]\\n{page_text}\"\n",
    "\n",
    "sections = detect_sections(pdf_text)\n",
    "print(f\"‚úÖ Detected {len(sections)} sections:\")\n",
    "for sec in sections:\n",
    "    print(f\"   - {sec['name']}\")\n",
    "\n",
    "#  STEP 4: Chunk text with section information\n",
    "print(\"\\nüìö Chunking text with section preservation...\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunked_data = []\n",
    "for section in sections:\n",
    "    section_chunks = splitter.split_text(section['content'])\n",
    "    for chunk in section_chunks:\n",
    "        if chunk.strip():\n",
    "            chunked_data.append({\n",
    "                'text': chunk,\n",
    "                'section': section['name']\n",
    "            })\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunked_data)} chunks\")\n",
    "\n",
    "#  STEP 5: Create semantic embeddings with sentence-transformers\n",
    "print(\"\\nüßÆ Creating semantic embeddings...\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "texts = [item['text'] for item in chunked_data]\n",
    "embeddings = embedding_model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "embeddings = embeddings.astype('float32')\n",
    "\n",
    "# Create FAISS index with cosine similarity\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "faiss.normalize_L2(embeddings)\n",
    "index.add(embeddings)\n",
    "print(f\"‚úÖ Built FAISS index with {index.ntotal} vectors (dim={dimension})\")\n",
    "\n",
    "#  STEP 6: Implement rank-based re-weighting\n",
    "def apply_rank_based_weighting(distances, k=5):\n",
    "    \"\"\"Apply exponential decay weighting based on rank\"\"\"\n",
    "    ranks = np.arange(1, k + 1)\n",
    "    weights = np.exp(-0.3 * (ranks - 1))\n",
    "    weights = weights / weights.sum()\n",
    "    return weights\n",
    "\n",
    "def get_top_k_chunks(query, k=5, section_filter=None):\n",
    "    \"\"\"Retrieve top-k chunks with rank-based weighting\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    if section_filter and section_filter != \"All\":\n",
    "        # Filter by section\n",
    "        filtered_indices = [i for i, item in enumerate(chunked_data) if item['section'] == section_filter]\n",
    "        if not filtered_indices:\n",
    "            return [], [], []\n",
    "        filtered_embeddings = np.array([embeddings[i] for i in filtered_indices])\n",
    "        temp_index = faiss.IndexFlatIP(filtered_embeddings.shape[1])\n",
    "        temp_index.add(filtered_embeddings)\n",
    "        distances, indices = temp_index.search(query_embedding, min(k, len(filtered_indices)))\n",
    "        original_indices = [filtered_indices[idx] for idx in indices[0]]\n",
    "        retrieved_chunks = [chunked_data[i] for i in original_indices]\n",
    "    else:\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        retrieved_chunks = [chunked_data[i] for i in indices[0]]\n",
    "    \n",
    "    weights = apply_rank_based_weighting(distances[0], k=len(retrieved_chunks))\n",
    "    return retrieved_chunks, weights, distances[0]\n",
    "\n",
    "#  STEP 7: Setup OpenAI for response generation\n",
    "print(\"\\nüîë Setting up OpenAI API...\")\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Get API key from Colab secrets or input\n",
    "try:\n",
    "    openai_key = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ Using API key from Colab secrets\")\n",
    "except:\n",
    "    openai_key = input(\"Please enter your OpenAI API key: \")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = openai_key\n",
    "\n",
    "import openai\n",
    "openai.api_key = openai_key\n",
    "\n",
    "def generate_response(retrieved_chunks, weights, query):\n",
    "    \"\"\"Generate response using rank-weighted chunks\"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, (chunk, weight) in enumerate(zip(retrieved_chunks, weights)):\n",
    "        importance = \"PRIMARY\" if i == 0 else \"SECONDARY\" if i < 3 else \"SUPPORTING\"\n",
    "        context_parts.append(\n",
    "            f\"[{importance} CONTEXT - Relevance: {weight:.2%}]\\n\"\n",
    "            f\"Section: {chunk['section']}\\n\"\n",
    "            f\"Content: {chunk['text']}\\n\"\n",
    "        )\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    prompt = f\"\"\"\n",
    "You are an expert research assistant analyzing a scientific paper. Use ONLY the provided context to answer the question.\n",
    "\n",
    "IMPORTANT:\n",
    "- Pay MORE attention to PRIMARY context (highest relevance)\n",
    "- Cite section names when providing information\n",
    "- Provide specific, factual answers based on the paper\n",
    "\n",
    "CONTEXT FROM SCIENTIFIC PAPER:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "Provide a clear, well-structured answer with section citations.\n",
    "\"\"\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert research assistant with high factual accuracy.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "#  STEP 8: Interactive querying with section filtering\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî¨ SCIENTIFIC PAPER RAG CHATBOT - READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAvailable sections:\", \", \".join(set([item['section'] for item in chunked_data])))\n",
    "print(\"\\nOptions:\")\n",
    "print(\"  - Ask a question directly\")\n",
    "print(\"  - Type 'filter:SectionName' to search only in that section\")\n",
    "print(\"  - Type 'sections' to list all sections\")\n",
    "print(\"  - Type 'exit' to quit\\n\")\n",
    "\n",
    "section_filter = None\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nüí¨ Your input: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"üëã Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    if user_input.lower() == \"sections\":\n",
    "        print(\"\\nüìë Available sections:\")\n",
    "        for sec in set([item['section'] for item in chunked_data]):\n",
    "            count = sum(1 for item in chunked_data if item['section'] == sec)\n",
    "            print(f\"   - {sec} ({count} chunks)\")\n",
    "        continue\n",
    "    \n",
    "    if user_input.lower().startswith(\"filter:\"):\n",
    "        section_filter = user_input[7:].strip().title()\n",
    "        print(f\"‚úÖ Filter set to: {section_filter}\")\n",
    "        continue\n",
    "    \n",
    "    # Process as a query\n",
    "    query = user_input\n",
    "    print(f\"\\nüîç Searching {'in section: ' + section_filter if section_filter else 'all sections'}...\")\n",
    "    \n",
    "    retrieved_chunks, weights, distances = get_top_k_chunks(query, k=5, section_filter=section_filter)\n",
    "    \n",
    "    if not retrieved_chunks:\n",
    "        print(f\"‚ö†Ô∏è No relevant content found\")\n",
    "    else:\n",
    "        print(f\"üìä Retrieved {len(retrieved_chunks)} chunks with rank-based weighting\\n\")\n",
    "        \n",
    "        # Show retrieval details\n",
    "        print(\"üìå Retrieved chunks:\")\n",
    "        for i, (chunk, weight) in enumerate(zip(retrieved_chunks, weights)):\n",
    "            print(f\"   {i+1}. [{chunk['section']}] - Weight: {weight:.2%}\")\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"\\n‚è≥ Generating answer...\")\n",
    "        answer = generate_response(retrieved_chunks, weights, query)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ ANSWER:\")\n",
    "        print(\"=\"*60)\n",
    "        print(answer)\n",
    "        print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
